# Sentiment Analysis Predictor

This project is a personal endeavor aimed at exploring the intricacies of Large Language Models (LLMs) and their underlying mechanisms. Through this initiative, I delve into how LLMs generate meaningful and accurate predictions by analyzing text data and uncovering the layers that contribute to their training process.

## Dataset

The dataset used for this project is the [Stanford Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/), a popular resource for sentiment analysis tasks.

# Stages of Building the LLM

## 1. Data Preparation & Sampling
   - Preparing the dataset to ensure clean, structured, and efficient processing for downstream tasks.

## 2. Tokenization
   - Breaking down text into tokens, the fundamental units of input for the model.

## 3. Token & Positional Embedding
   - Mapping tokens into vector spaces and incorporating positional context to enhance understanding of sequential data.

## 4. Attention Mechanism
   - Implementing the core component of modern LLMs, enabling the model to focus on relevant parts of the input.

## 5. LLM Architecture
   - Structuring and fine-tuning the architecture to optimize performance for sentiment prediction tasks.
